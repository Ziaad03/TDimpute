{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68a9d41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "total 4\n",
      "drwxr-xr-x 1 root root 4096 Nov 17 14:29 sample_data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "!ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd494cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_batch(dataset1, batch_size_1, step, ind): # dataset, 10, 0, [0,1,2,3,4,5,6,7,8,9]\n",
    "    start = step * batch_size_1 # 0, 1\n",
    "    end = ((step + 1) * batch_size_1) # 10, 20\n",
    "    sel_ind = ind[start:end] # [0,1,2,3,4,5,6,7,8,9], [10,11,12,13,14,15,16,17,18,19]\n",
    "    newdataset1 = dataset1[sel_ind, :] # dataset[[0,1,2,3,4,5,6,7,8,9], :], dataset[[10,11,12,13,14,15,16,17,18,19], :]\n",
    "    return newdataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49c21449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(drop_prob, dataset_train, dataset_test, sav=True, checkpoint_file='default.ckpt'):\n",
    "    input_image = tf.placeholder(tf.float32, batch_shape_input, name='input_image')\n",
    "    is_training = tf.placeholder(tf.bool) # to control dropout\n",
    "    scale = 0. # for the L2 regularization\n",
    "    with tf.variable_scope('autoencoder') as scope:\n",
    "        fc_1 = tf.layers.dense(inputs=input_image, units=4000, kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=scale))\n",
    "        fc_1_out = tf.nn.sigmoid(fc_1)\n",
    "        fc_1_dropout = tf.layers.dropout(inputs=fc_1_out, rate=drop_prob, training=is_training)\n",
    "        fc_2_dropout = tf.layers.dense(inputs=fc_1_dropout, units=19027)  # 46744 (the original prediction)\n",
    "        fc_2_out = tf.nn.sigmoid(fc_2_dropout)\n",
    "        reconstructed_image = fc_2_out  # fc_2_dropout (the predictored gene expression values after normalization)\n",
    "\n",
    "    original = tf.placeholder(tf.float32, batch_shape_output, name='original')\n",
    "    loss = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(reconstructed_image, original))))\n",
    "    l2_loss = tf.losses.get_regularization_loss()\n",
    "    optimizer = tf.train.AdamOptimizer(lr).minimize(loss + l2_loss) # runs backpropagation and updates the weights\n",
    "\n",
    "    # set up the training variables and environment\n",
    "    init = tf.global_variables_initializer() # initialize all variables in the graph (weights and biases)\n",
    "    saver = tf.train.Saver()\n",
    "    start = time.time()\n",
    "    loss_val_list_train = 0\n",
    "    loss_val_list_test = 0\n",
    "\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    with tf.Session(config=config) as session: # open the session to run the computation graph\n",
    "        session.run(init) # give random initial values to all variables\n",
    "\n",
    "        dataset_size_train = dataset_train.shape[0]\n",
    "        dataset_size_test = dataset_test.shape[0]\n",
    "        print(\"Dataset size for training:\", dataset_size_train)\n",
    "        print(\"Dataset size for test:\", dataset_size_test)\n",
    "        num_iters = (num_epochs * dataset_size_train) // batch_size\n",
    "        print(\"Num iters:\", num_iters)\n",
    "        ind_train = []\n",
    "\n",
    "        # Generate for every epoch a random training samples,\n",
    "        for i in range(num_epochs):\n",
    "            ind_train = np.append(ind_train, np.random.permutation(np.arange(dataset_size_train)))\n",
    "        ind_train = np.asarray(ind_train).astype(\"int32\")\n",
    "\n",
    "        total_cost_train = 0.\n",
    "        num_batchs = dataset_size_train // batch_size  # \"//\" => int()\n",
    "        for step in range(num_iters): # num of iteration cover all batches and epochs\n",
    "            ############### train section  ###########################\n",
    "            temp = get_next_batch(dataset_train, batch_size, step, ind_train)\n",
    "            train_batch = np.asarray(temp).astype(\"float32\")\n",
    "\n",
    "            train_loss_val, _ = session.run([loss, optimizer],  # compute loss and do the optimization (backpropagation) for one batch\n",
    "                                            feed_dict={input_image: train_batch[:, 19027:],\n",
    "                                                       original: train_batch[:, :19027],\n",
    "                                                       is_training: True})\n",
    "            loss_val_list_train = np.append(loss_val_list_train, train_loss_val) # append the training loss value for this batch\n",
    "            total_cost_train += train_loss_val # accumulate the training loss value\n",
    "\n",
    "            print_epochs = 10\n",
    "            if step % (num_batchs * print_epochs) == 0:  # by epoch, num_batchs * batch_size = dataset_train_size\n",
    "                ############### test section  ###########################\n",
    "                dataset_test = np.asarray(dataset_test).astype(\"float32\")\n",
    "                test_loss_val = session.run(loss, feed_dict={input_image: dataset_test[:, 19027:],\n",
    "                                                             original: dataset_test[:, :19027],\n",
    "                                                             is_training: False})\n",
    "                loss_val_list_test = np.append(loss_val_list_test, test_loss_val)\n",
    "\n",
    "                reconstruct = session.run(reconstructed_image,\n",
    "                                          feed_dict={input_image: dataset_test[:, 19027:], is_training: False}) # get the predicted gene expression values for test set\n",
    "                nz = dataset_test[:, :19027].shape[0] * dataset_test[:, :19027].shape[1] # total number of GE values in test set (no of samples * 19K )\n",
    "                diff_mat = ((reconstruct  - dataset_test[:, :19027] ) * 16.5) ** 2\n",
    "                loss_test = np.sqrt(np.sum(diff_mat) / nz) # compute the RMSE \n",
    "\n",
    "                print('RMSE loss by train_data_size: ', step, \"/\", num_iters,\n",
    "                      total_cost_train / (num_batchs * print_epochs),\n",
    "                      test_loss_val, loss_test)\n",
    "                # print('RMSE loss by train_data_size: ', step, \"/\", num_iters, total_cost_train / num_batchs,\n",
    "                #       total_cost_validation / num_batchs)\n",
    "                #                 print('new loss: ', step, \"/\", num_iters, train_loss_val, valid_loss_val)\n",
    "                total_cost_train = 0.\n",
    "                total_cost_validation = 0.\n",
    "\n",
    "        ############### test section  ###########################\n",
    "        dataset_test = np.asarray(dataset_test).astype(\"float32\")\n",
    "        test_loss_val = session.run(loss, feed_dict={input_image: dataset_test[:, 19027:],\n",
    "                                                     original: dataset_test[:, :19027],\n",
    "                                                     is_training: False})\n",
    "        loss_val_list_test = np.append(loss_val_list_test, test_loss_val)\n",
    "\n",
    "        reconstruct = session.run(reconstructed_image,\n",
    "                                  feed_dict={input_image: dataset_test[:, 19027:], is_training: False})\n",
    "        nz = dataset_test[:, :19027].shape[0] * dataset_test[:, :19027].shape[1]\n",
    "        diff_mat = ((reconstruct  - dataset_test[:, :19027] ) * 16.5) ** 2\n",
    "        loss_test = np.sqrt(np.sum(diff_mat) / nz)\n",
    "\n",
    "        print('RMSE loss by train_data_size: ', step, \"/\", num_iters, test_loss_val, loss_test)\n",
    "        # print('RMSE loss by train_data_size: ', step, \"/\", num_iters, total_cost_train / num_batchs,\n",
    "        #       total_cost_validation / num_batchs)\n",
    "        #                 print('new loss: ', step, \"/\", num_iters, train_loss_val, valid_loss_val)\n",
    "    #             save_path = saver.save(session, checkpoint_file)  # \" checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    #             print((\"Model saved in file: %s\" % save_path))\n",
    "\n",
    "    end = time.time()\n",
    "    el = end - start\n",
    "    print((\"Time elapsed %f\" % el))\n",
    "    return (loss_val_list_train, loss_val_list_test, loss_test, reconstruct)\n",
    "\t\n",
    "#############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5996aeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py', '-f', '/root/.local/share/jupyter/runtime/kernel-79ed4ec1-6b15-43ba-80b4-e789b773e945.json']\n",
      "Using GPU: 0\n",
      "epos\\graad-porj\\TDimpute2\\RNA_DNA_combine.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:18: SyntaxWarning: invalid escape sequence '\\g'\n",
      "<>:18: SyntaxWarning: invalid escape sequence '\\g'\n",
      "/tmp/ipython-input-1401854209.py:18: SyntaxWarning: invalid escape sequence '\\g'\n",
      "  full_dataset_path =\"D:\\repos\\graad-porj\\TDimpute2\\RNA_DNA_combine.csv\"\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\repos\\\\graad-porj\\\\TDimpute2\\\\RNA_DNA_combine.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1401854209.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m# shuffle_cancer = pd.read_csv(datadir+'/full_datasets/' + cancertype + str(int(missing_perc * 100)) + '_' + str(sample_count) + '.csv',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m#     delimiter=',', index_col=0, header=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mshuffle_cancer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_dataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# loop around 4 times getting chunk of 2k rows and do normalization and concateate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\repos\\\\graad-porj\\\\TDimpute2\\\\RNA_DNA_combine.csv'"
     ]
    }
   ],
   "source": [
    "import getopt\n",
    "import sys\n",
    "print(sys.argv)\n",
    "\n",
    "# # datadir = sys.argv[1] #e.g.,'/data0/zhoux'\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = sys.argv[1] # set the GPU id\n",
    "# cancer_names = [sys.argv[2]] #smaple size greater than 200 , take a cancer name as input (dont know what is its usage yet)\n",
    "# # cancer_names = ['LUSC', 'KIRC', 'CESC', 'STAD', 'SARC', 'COAD','KIRP', 'LUAD', 'BLCA', 'BRCA','HNSC','LGG_','PRAD','THCA','SKCM', 'LIHC']\n",
    "# full_dataset_path = sys.argv[3]\n",
    "# imputed_dataset_path = sys.argv[4]\n",
    "# datadir = os.path.dirname(os.path.abspath(full_dataset_path))\n",
    "\n",
    "# Kaggle GPU setup\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Dataset paths\n",
    "datadir = os.getcwd()\n",
    "full_dataset_path =r\"D:\\repos\\graad-porj\\TDimpute2\\RNA_DNA_combine.csv\"\n",
    "imputed_dataset_path = \"imputed_RNA_DNA_combine.csv\"\n",
    "\n",
    "# Cancer type\n",
    "cancer_names = [\"BRCA\"]\n",
    "\n",
    "print(\"Using GPU:\", os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "print(\"Train/Test file:\", full_dataset_path)\n",
    "\n",
    "sample_size = 1 # was 5 \n",
    "loss_list = np.zeros([16, 5, sample_size]) # 16 cancers, 5 missing rates, sample_size repeats\n",
    "loss_summary = np.zeros([16, 5]) # 16 cancers, 5 missing rates\n",
    "cancer_c = 0\n",
    "for cancertype in cancer_names:\n",
    "    perc = 0\n",
    "    for missing_perc in [0.5]:   #  [0.1,0.3,0.5,0.7,0.9] # the percentage control the split between train and test data\n",
    "        for sample_count in range(1,sample_size+1): # What is sample_count used for?\n",
    "            # shuffle_cancer = pd.read_csv(datadir+'/full_datasets/' + cancertype + str(int(missing_perc * 100)) + '_' + str(sample_count) + '.csv',\n",
    "            #     delimiter=',', index_col=0, header=0)\n",
    "            shuffle_cancer = pd.read_csv(full_dataset_path, delimiter=',', index_col=0, header=0, nrows=1000)\n",
    "\n",
    "            # loop around 4 times getting chunk of 2k rows and do normalization and concateate\n",
    "            \n",
    "            print('name:', cancertype, ' missing rate:', missing_perc, ' data size:',shuffle_cancer.shape)\n",
    "            ########################Create set for training and testing\n",
    "            ## 16.5 is for gene expression normalization // linearly scaled to [0, 1], the same as the DNA methylation data (beta values)\n",
    "            aa = np.concatenate((shuffle_cancer.values[:, :19027] / 16.5, shuffle_cancer.values[:, 19027:]), axis=1)\n",
    "            shuffle_cancer = pd.DataFrame(aa, index=shuffle_cancer.index, columns=shuffle_cancer.columns)\n",
    "            RDNA = shuffle_cancer.values # RDNA is now a matrix contain just numeric value the value of feature j for sample i.\n",
    "            test_data = RDNA[0:int(RDNA.shape[0] * missing_perc), :]\n",
    "            train_data = RDNA[int(RDNA.shape[0] * missing_perc):, :]\n",
    "            print('train datasize:', train_data.shape[0], ' test datasize: ', test_data.shape[0])\n",
    "\n",
    "            num_epochs = 100\n",
    "            batch_size = 16\n",
    "            lr = 0.0001  # learning_rate = 0.1\n",
    "            feature_size = RDNA.shape[1]  # 10595 #17176 #(8856, 109995)\n",
    "            drop_prob = 0.\n",
    "            batch_shape_input = (None, 27717)  # (128, 11853)\n",
    "            batch_shape_output = (None, 19027)\n",
    "            tf.reset_default_graph()\n",
    "            loss_val_list_train, loss_val_list_test, loss_test, reconstruct = train(drop_prob, train_data, test_data,\n",
    "                                                                                    sav=True, checkpoint_file=datadir +\"/checkpoints/\"+ cancertype+str(missing_perc*100)+'_'+str(sample_count)+ '_imputationmodel.ckpt')\n",
    "\n",
    "            imputed_data = np.concatenate([reconstruct*16.5, train_data[:, :19027]*16.5], axis=0)\n",
    "            RNA_txt = pd.DataFrame(imputed_data[:, :19027], index=shuffle_cancer.index,\n",
    "                                   columns=shuffle_cancer.columns[:19027])\n",
    "            # RNA_txt.to_csv(datadir+'/imputed_data/TDimpute_without_tf_'+cancertype+str(missing_perc*100)+'_'+str(sample_count)+'.csv')\n",
    "            RNA_txt.to_csv(imputed_dataset_path)\n",
    "\n",
    "            loss_list[cancer_c, perc, sample_count-1] = loss_test #loss_val_list_test[-1]\n",
    "\n",
    "        perc = perc + 1\n",
    "        np.set_printoptions(precision=3)\n",
    "        print(np.array([np.mean(loss_list[cancer_c,i,:]) for i in range(0,5)] ))\n",
    "    loss_summary[cancer_c,:] = np.array( [np.mean(loss_list[cancer_c,i,:]) for i in range(0,5)] )\n",
    "    cancer_c = cancer_c + 1\n",
    "\n",
    "# print(loss_list)\n",
    "print('RMSE by cancer (averaged by sampling times):')\n",
    "print(loss_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7a95dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
